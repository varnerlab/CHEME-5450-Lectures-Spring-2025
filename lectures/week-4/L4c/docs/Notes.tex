\documentclass{article}[11pt]
\usepackage{fullpage,graphicx, setspace, latexsym, cite,amsmath,amssymb,xcolor,subfigure}
%\usepackage{epstopdf}
%\DeclareGraphicsExtensions{.pdf,.eps,.png,.jpg,.mps} 
\usepackage{amssymb} %maths
\usepackage{amsmath} %maths
\usepackage{amsthm, comment}
\usepackage[round,comma,sort,numbers]{natbib}

% \bibliographystyle{plain}
\bibliographystyle{plos2015}

\newtheorem{theorem}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{defn}{Definition}
\newtheorem{ex}{Example}
\usepackage{float}

\newcommand*{\underuparrow}[1]{\underset{\uparrow}{#1}}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[dvipsnames]{xcolor}
\usepackage{algorithmicx}
\usepackage{algorithm} %http://ctan.org/pkg/algorithms
\usepackage{algpseudocode} %http://ctan.org/pkg/algorithmicx
\usepackage{enumitem}
\usepackage{simplemargins}
\usepackage{hyperref}

\usepackage{mdframed}
\definecolor{lgray}{rgb}{0.92,0.92,0.92}
\definecolor{lsalmon}{rgb}{0.9921568627450981,0.9411764705882353, 0.9254901960784314}

\renewcommand{\bibnumfmt}[1]{#1.}
\setlist{noitemsep} % or \setlist{noitemsep} to leave space around whole list
\setallmargins{1in}
\linespread{1.1}

\newcommand{\brows}[1]{%
  \begin{bmatrix}
  \begin{array}{@{\protect\rotvert\;}c@{\;\protect\rotvert}}
  #1
  \end{array}
  \end{bmatrix}
}
\newcommand{\rotvert}{\rotatebox[origin=c]{90}{$\vert$}}
\newcommand{\rowsvdots}{\multicolumn{1}{@{}c@{}}{\vdots}}


\def\R{\mathbb{R}}
\def\Eps{\mathcal{E}}
\def\E{\mathbb{E}}
\def\V{\mathbb{V}}
\def\F{\mathcal{F}}
\def\G{\mathcal{G}}
\def\H{\mathcal{H}}
\def\S{\mathcal{S}}
\def\D{\mathcal{D}}
\def\P{\mathbb{P}}
\def\1{\mathbf{1}}
\def\n{\nappa}
\def\h{\mathbf{w}}
\def\v{\mathbf{v}}
\def\x{\mathbf{x}}
\def\X{\mathcal{X}}
\def\Y{\mathcal{Y}}
\def\eps{\epsilon}
\def\y{\mathbf{y}}
\def\e{\mathbf{e}}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   % \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \setlength{\headsep}{10mm}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CHEME 5820: Machine Learning for Engineers
   \hfill Spring 2025} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   \noindent{\bf Disclaimer}: {\it These notes have not been subjected to the
   usual scrutiny reserved for formal publications. }
   \vspace*{4mm}
}

\begin{document}
\lecture{4c}{Metabolic Networks and the Stoichiometric Matrix}{Jeffrey Varner}{}

\begin{mdframed}[backgroundcolor=lgray]
    In this lecture, we will discuss the following topics:
    \begin{itemize}[leftmargin=16pt]
        \item{Introduction to metabolic networks}
        \item{The stoichiometric matrix}
        \item{The rank of the stoichiometric matrix}
        \item{The null space of the stoichiometric matrix}
    \end{itemize}
 \end{mdframed}

\section{Introduction}
In this lecture we'll introduce metabolic networks and the stoichiometric matrix, which is a fundamental tool in systems biology for modeling and analyzing biochemical reaction systems.
Metabolic networks are complex systems of enzyme catalyzed biochemical reactions that convert nutrients into energy and building blocks for cellular components, or products of interest.
These chemical reaction networks can be mathematically represented using stoichiometric matrices. 
A stoichiometric matrix provides a systematic way to represent the flow of material through various biochemical reactions, where rows correspond to chemical species (metabolites) and columns correspond to reactions. 
This matrix is essential for constraint-based modeling and flux balance analysis, allowing researchers to analyze and predict metabolic behavior under different conditions. 
Today, we'll introduce the stoichiometric matrix and discuss its properties, including rank and null space.

\section{Stoichiometric Matrix}
Suppose we have a set of chemical reactions $\mathcal{R}$ involving the chemical species (metabolite) set $\mathcal{M}$ occurring in some volume $V$.
This can be a physical volume, such as a test tube, or a biological volume, such as a cell or even a logical volume, such as a compartment in a model.
We can represent this metabolic network using the stoichiometric matrix $\mathbf{S}$ (Defn \ref{defn-stoichiometric-matrix}):

\begin{defn}[Stoichiometric Matrix]\label{defn-stoichiometric-matrix}
The stoichiometric matrix is a $\mathbf{S}\in\mathbb{R}^{|\mathcal{M}|\times|\mathcal{R}|}$ matrix that holds the stoichiometric coefficients $\sigma_{ij}\in\mathbf{S}$ such that:
   \begin{itemize}[leftmargin=16pt]
      \item{$\sigma_{ij}>0$: Chemical species $i$ is \textit{produced} by reaction $j$. Species $i$ is a product of reaction $j$.}
      \item{$\sigma_{ij} = 0$: Chemical species $i$ is not connected with reaction $j$}
      \item{$\sigma_{ij}<0$: Chemical species $i$ is \textit{consumed} by reaction $j$. Species $i$ is a reactant of reaction $j$.}
   \end{itemize}
   The stoichiometric matrix is a digital representation of the metabolic network, and it can be used to analyze the structure of the network, 
   as well as to estimate operational values, such as the metabolic fluxes.
\end{defn}

The stoichiometric matrix $\mathbf{S}$ is the digital representation of the metabolic network, and it can be used to analyze some of the structural properties of the network.
There is a large body of literature on the analysis of the structure of metabolic networks using various decomposition approaches of stoichiometric matrices. 
Let's look at a few of these, starting with the connectivity of the network, the rank of the stoichiometric matrix, 
and finally the decomposition using singular value decomposition (SVD) \cite{Famili:2003aa}.

\section{Connectivity of the Network}
The connectivity of a metabolic network, and the connectivity distribution, gives us information about the network's structure, e.g., 
the number of reactions that are connected to a given metabolite, or the number of metabolites that are connected to a given reaction, etc.
We compute the connectivity of the network using the binary stoichiometric matrix $\bar{\mathbf{S}}$ (Defn. \ref{defn-binary-stoichiometric-matrix}).

\begin{defn}[Binary stoichiometric Matrix]\label{defn-binary-stoichiometric-matrix}
Suppose we have a stoichiometric matrix $\mathbf{S}$, the binary stoichiometric matrix $\bar{\mathbf{S}}$ is a matrix with the same dimensions as $\mathbf{S}$, 
where the elements are binary, i.e., $\bar{\sigma}_{ij} = 1$ if $\sigma_{ij}\neq 0$, and $\bar{\sigma}_{ij} = 0$ if $\sigma_{ij} = 0$.
Matrix products of $\bar{\mathbf{S}}$ with its transpose $\bar{\mathbf{S}}^{\top}$ (or vice-versa) give us connectivity information about the network.
\end{defn}

\paragraph*{Metabolite connectivity} The metabolite connectivity matrix, defined as $\mathbf{C}_{m} \equiv \bar{\mathbf{S}}\bar{\mathbf{S}}^{\top}$, is an $|\mathcal{M}|\times|\mathcal{M}|$ symmetric array with the following features:
The elements along the central diagonal $c_{ii}\in\mathbf{C}_{m}$ are the total number of reactions a particular metabolite participates in. 
However. because we removed the directionality when we computed the binary stoichiometric matrix, we have no information about whether the participation is a reactant or product.
The off-diagonal elements $c_{ij}\in\mathbf{C}_{m}$ where $i\neq{j}$ describe how many reactions metabolite $i$ has in common with metabolite $j$, i.e., the number of joints reactions for the pair.

\paragraph*{Reaction connectivity} The reaction connectivity matrix, defined as $\mathbf{C}_{r} \equiv \bar{\mathbf{S}}^{\top}\bar{\mathbf{S}}$, is an $|\mathcal{R}|\times|\mathcal{R}|$ symmetric array with the following features.
The elements along the central diagonal $c_{ii}\in\mathbf{C}_{r}$ are the total number of reactants and products of a particular reaction. 
However, because we removed the directionality when we computed the binary stoichiometric matrix, we have no information about the number of reactants or products, just the total participation number for a reaction.
The off-diagonal elements of the reaction matrix $c_{ij}\in\mathbf{C}_{r}$ where $i\neq{j}$ describe how many metabolites are shared between reaction $i$ and $j$, i.e., the number of joint metabolites for the pair.

\section{Eigendecomposition}
In addiiton to looking at the connectivity of the network, we can also analyze the structure of the network using eigendecomposition of the stoichiometric matrix.
However, the stoichiometric matrix is not square, so we cannot use the standard eigendecomposition. 
Instead, we can use singular value decomposition (SVD) to analyze the structure of the stoichiometric matrix. 
Let's work our way through the eigendecomposition of a square matrix, and then we will apply SVD to the stoichiometric matrix.

Suppose we have a \texttt{square} matrix $\mathbf{A}\in\mathbb{R}^{m\times{m}}$ which could be a measurement dataset, e.g., the columns of $\mathbf{A}$ represent feature 
vectors $\mathbf{x}_{1},\dots,\mathbf{x}_{m}$ or an adjacency array in a graph with $m$ nodes, etc. Eigenvalue-eigenvector problems involve finding a set of scalar values $\left\{\lambda_{1},\dots,\lambda_{m}\right\}$ called 
\href{https://mathworld.wolfram.com/Eigenvalue.html}{eigenvalues} and a set of linearly independent vectors 
$\left\{\mathbf{v}_{1},\dots,\mathbf{v}_{m}\right\}$ called \href{https://mathworld.wolfram.com/Eigenvector.html}{eigenvectors} such that:
\begin{equation}
\mathbf{A}\cdot\mathbf{v}_{j} = \lambda_{j}\mathbf{v}_{j}\qquad{j=1,2,\dots,m}
\end{equation}
where $\mathbf{v}\in\mathbb{C}^{m}$ and $\lambda\in\mathbb{C}$. We can put the eigenvalues and eigenvectors together in matrix-vector form, which gives us an interesting matrix decomposition:
\begin{equation}
\mathbf{A} = \mathbf{V}\cdot\text{diag}(\lambda)\cdot\mathbf{V}^{-1}
\end{equation}
where $\mathbf{V}$ denotes the matrix of eigenvectors, where the eigenvectors form the columns of the matrix $\mathbf{V}$, $\text{diag}(\lambda)$ denotes a diagonal matrix with the eigenvalues along the main diagonal, 
and $\mathbf{V}^{-1}$ denotes the inverse of the eigenvalue matrix.
The eigendecomposition of a matrix is a powerful tool that can be used to analyze data and systems in many areas of mathematics, engineering, and physics.
Let's discuss some of the interpretations of eigendecomposition and data reduction, which is an interesting application of eigendecomposition. Then, we will discuss how to compute the eigendecomposition of a matrix.

\subsection{Interpretation of Eigendecomposition}
Eigenvectors represent fundamental directions of the matrix $\mathbf{A}$. For the linear transformation defined by a matrix $\mathbf{A}$, eigenvectors are the only vectors that do not change direction during the transformation. 
Thus, if we think about the matrix $\mathbf{A}$ as a machine, we put the eigenvector $\mathbf{v}_{\star}$ into the machine, and we get back the same eigenvector $\mathbf{v}_{\star}$ multiplied by a scalar, the eigenvalue $\lambda_{\star}$.
On the other hand, eigenvalues are scale factors for their eigenvector. An eigenvalue is a scalar that indicates how much a corresponding eigenvector is stretched or compressed during a linear transformation represented by the matrix $\mathbf{A}$.
We can use the eigendecomposition to diagonalize the matrix $\mathbf{A}$, i.e., transform the matrix into a diagonal form where the eigenvalues lie along the main diagonal. To see this, solve the eigendecomposition for the $\text{diag}(\lambda) = \mathbf{V}^{-1}\cdot\mathbf{A}\cdot\mathbf{V}$. 
We can also use the eigenvalues to classify a matrix $\mathbf{A}$ as positive (semi)definite or negative (semi)definite (which will be handy later). 
Further, suppose the matrix $\mathbf{A}$ is symmetric, and all entries are positive. In that case, all the eigenvalues will be real-valued, and the eigenvectors will be orthogonal (super handy properties, as we shall soon see).
Finally, eigenvectors represent the most critical directions in the data or system, and eigenvalues (or functions of them) represent their importance. However, the eigendecomposition is not always possible, e.g., for non-square matrices or matrices that are not diagonalizable (which may be the case with repeated eigenvalues). Finally, it may seem rare to encounter a square symmetric real-valued matrix in practice, e.g., stoichiometric matrices are (often) not square or symmetric; actually, this is not the case in engineering systems and data. Let's dig into the properties of symmetric real-valued matrices and introduce the covariance matrix, a common example of a symmetric real-valued matrix we will encounter in many applications.

\subsection{Symmetric Real Matrices}
The eigendecomposition of a symmetric real matrix $\mathbf{A}\in\mathbb{R}^{m\times{m}}$ has some unique properties. 
First, all the eigenvalues $\left\{\lambda_{1},\lambda_{2},\dots,\lambda_{m}\right\}$ of the matrix $\mathbf{A}$ are real-valued.
Next, the eigenvectors $\left\{\mathbf{v}_{1},\mathbf{v}_{2},\dots,\mathbf{v}_{m}\right\}$ of the matrix $\mathbf{A}$ are orthogonal, i.e., $\left<\mathbf{v}_{i},\mathbf{v}_{j}\right> = 0$ for $i\neq{j}$.
Finally, the (normalized) eigenvectors $\mathbf{v}_{j}/\norm{\mathbf{v}_{j}}$ of a symmetric real-valued matrix 
form an orthonormal basis for the space spanned by the matrix $\mathbf{A}$ such that:
\begin{equation}
\left<\hat{\mathbf{v}}_{i},\hat{\mathbf{v}}_{j}\right> = \delta_{ij}\qquad\text{for}\quad{i,j\in\mathbf{A}}
\end{equation}
where $\delta_{ij}$ is the \href{https://en.wikipedia.org/wiki/Kronecker_delta}{Kronecker delta function}. The eigendecomposition of a symmetric real-valued matrix is a powerful tool that can be used to analyze data and systems in many areas of mathematics, engineering, and physics. For example, eigenvectors of a real-symmetric matrix form an orthogonal (orthonormal) basis for the space spanned by the matrix $\mathbf{A}$. Thus, any vector $\mathbf{x}\in\mathbb{R}^{m}$  in that space, i.e., a solution vector can be expressed as a linear combination of the eigenvectors of the matrix $\mathbf{A}$
i.e., $\mathbf{x} = \sum_{i=1}^{m}c_{i}\mathbf{v}_{i}$, where $c_{i}$ are the coefficients of the linear combination.
Further, the eigenvectors of a symmetric real-valued matrix can be used to reduce the dimensionality of a dataset (which we will discuss later).

\subsubsection*{Covariance Matrix}
The covariance matrix of a dataset is an example of a symmetric real matrix that we will encounter in various applications.
The covariance matrix is a square matrix that summarizes the variance and covariance of the features in the dataset.
Suppose we have a dataset $\D = \left\{\mathbf{x}_{1},\mathbf{x}_{2},\dots,\mathbf{x}_{n}\right\}$ where each $\mathbf{x}_{i}\in\mathbb{R}^{m}$ is a feature vector.
The covariance of feature vectors $i$ and $j$, denoted as $\text{cov}\left(\mathbf{x}_{i},\mathbf{x}_{j}\right)$, is an $\mathbf{\Sigma}\in\mathbb{R}^{n\times{n}}$ 
real-valued symmetric matrix $\mathbf{\Sigma}\in\R^{n\times{n}}$ with elements: 
\begin{equation}
    \Sigma_{ij} = \text{cov}\left(\mathbf{x}_{i},\mathbf{x}_{j}\right) = \sigma_{i}\,\sigma_{j}\,\rho_{ij}\qquad\text{for}\quad{i,j \in \mathcal{D}}
\end{equation}
where $\sigma_{i}$ denote the standard deviation of the feature vector $\mathbf{x}_{i}$, $\sigma_{j}$ denote the standard deviation of the 
feature vector $\mathbf{x}_{j}$, and $\rho_{ij}$ denotes the correlation between features $i$ and $j$ in the dataset $\D$. The correlation is given by:
\begin{equation}
\rho_{ij} = \frac{\mathbb{E}(\mathbf{x}_{i}-\mu_{i})\cdot\mathbb{E}(\mathbf{x}_{j} - \mu_{j})}{\sigma_{i}\sigma_{j}}\qquad\text{for}\quad{i,j \in \mathcal{D}}
\end{equation}
where $\mathbb{E}(\cdot)$ denotes the expected value, and $\mu_{i}$ denotes the mean of the feature vector $\mathbf{x}_{i}$.
The diagonal elements of the covariance matrix $\Sigma_{ii}\in\mathbf{\Sigma}$ are the variances of features $i$,
while the off-diagonal elements $\Sigma_{ij}\in\mathbf{\Sigma}$ for $i\neq{j}$ measure the relationship between features 
$i$ and $j$ in the dataset $\mathcal{D}$. Interestingly, we can use the eigendecomposition of the covariance matrix for data reduction, 
i.e., factor the dataset $\D$ into a set of weighted (increasingly important) features. For example, if $n\gg{2}$, 
we can use the eigendecomposition of the covariance to reduce the dimensionality of the dataset $\D$ to $2$ or $3$ dimensions, which can be visualized.
We can also use the eigendecomposition of the covariance to find the most important features in the dataset, which can be used for clustering, classification, or other machine-learning tasks.

\section{Computing the Eigendecomposition of a Matrix}
There are several techniques to compute the eigendecomposition of a matrix. 
The most common method is power iteration, an iterative algorithm that finds the eigenvector corresponding to the largest eigenvalue of a matrix. The power iteration method is simple and easy to implement. Still, it may not converge to the desired eigenvector if the matrix is ill-conditioned or has multiple eigenvalues of the same magnitude.
Alternatively, we can use the \href{https://en.wikipedia.org/wiki/QR_algorithm}{QR iteration algorithm} to find all the eigenvalues and eigenvectors.
We'll only briefly discuss the QR iteration algorithm here, but it is a powerful method for computing the eigendecomposition of a matrix.

\subsection{Power Iteration}
The \href{https://en.wikipedia.org/wiki/Power_iteration}{power iteration method} 
is an iterative algorithm to compute the largest eigenvalue and its corresponding eigenvector of a square (real) matrix; we'll consider only real-valued matrices here, 
but this approach can also be used for matrices with complex entries. 

\begin{mdframed}
The power iteration method consists of two phases:
\begin{itemize}[leftmargin=16pt]
\item{\textbf{Phase 1: Eigenvector}: Suppose we have a real-valued square diagonalizable matrix $\mathbf{A}\in\mathbb{R}^{m\times{m}}$ whose eigenvalues have the property $|\lambda_{1}|\geq|\lambda_{2}|\dots\geq|\lambda_{m}|$. 
   Then, the eigenvector $\mathbf{v}_{1}\in\mathbb{C}^{m}$ which corresponds to the largest eigenvalue $\lambda_{1}\in\mathbb{C}$ can be (iteratively) estimated as:
   \begin{equation}
      \mathbf{v}_{1}^{(k+1)} = \frac{\mathbf{A}\mathbf{v}_{1}^{(k)}}{\Vert \mathbf{A}\mathbf{v}_{1}^{(k)} \Vert}\quad{k=0,1,2\dots}
   \end{equation}
   where $\lVert \star \rVert$ denotes the \href{https://mathworld.wolfram.com/L2-Norm.html}{L2 (Euclidean) vector norm}. 
   The \href{https://en.wikipedia.org/wiki/Power_iteration}{power iteration method} converges to a value for the eigenvector as $k\rightarrow\infty$ 
   when a few properties are true, namely, $|\lambda_{1}|/|\lambda_{2}| < 1$ (which is unknown beforehand), and we pick an appropriate initial guess for $\mathbf{v}_{1}$ (in our case, a random vector will work)
}
\item{\textbf{Phase 2: Eigenvalue}: Once we have an estimate for the eigenvector $\hat{\mathbf{v}}_{1}$, we can estimate the corresponding eigenvalue $\hat{\lambda}_{1}$ using \href{https://en.wikipedia.org/wiki/Rayleigh_quotient}{the Rayleigh quotient}. 
   This argument proceeds from the definition of the eigenvalues and eigenvectors. We know, from the definition of eigenvalue-eigenvector pairs, that:
   \begin{equation}
      \mathbf{A}\hat{\mathbf{v}}_{1} - \hat{\lambda}_{1}\hat{\mathbf{v}}_{1}\simeq{0}
   \end{equation}
where we use the $\simeq$ symbol because we don't have the true eigenvector $\mathbf{v}_{1}$, only an estimate of it. To solve this expression for the (estimated) eigenvalue $\hat{\lambda}_{1}$, we multiply through by the transpose of the eigenvector and solve for the eigenvalue:
   \begin{equation}
      \hat{\lambda}_{1} \simeq \frac{\hat{\mathbf{v}}_{1}^{\top}\mathbf{A}\hat{\mathbf{v}}_{1}}{\hat{\mathbf{v}}_{1}^{\top}\hat{\mathbf{v}}_{1}} = \frac{\left<\mathbf{A}\hat{\mathbf{v}}_{1},\hat{\mathbf{v}}_{1}\right>}{\left<\hat{\mathbf{v}}_{1},\hat{\mathbf{v}}_{1}\right>}
   \end{equation}
where $\left<\star,\star\right>$ denotes \href{https://mathworld.wolfram.com/InnerProduct.html}{an inner product}}
While simple to implement, the \href{https://en.wikipedia.org/wiki/Power_iteration}{power iteration method} may exhibit slow convergence, mainly when the largest eigenvalue is close in magnitude to other eigenvalues, i.e., $|\lambda_{1}|/|\lambda_{2}| \sim 1$.
\end{itemize}
\end{mdframed}
The power iteration method has several notable applications. For example, likely the most famous application is the \href{https://en.wikipedia.org/wiki/PageRank}{Google PageRank algorithm}.
Google's PageRank algorithm, which uses power iteration, utilizes the dominant eigenvalue and its corresponding eigenvector to assess the importance of web pages within a network.
For a deeper discussion of the PageRank algorithm, see Brezinski and Redivo-Zaglia \citep{Brezinski:2006}. Psuedo code for the power iteration method is shown in Algorithm \ref{alg:power_iteration}.
\begin{algorithm}[H]
   \caption{Power Iteration Method}\label{alg:power_iteration}
\begin{algorithmic}[1]
   \State{\textbf{Input}: Matrix $A \in \mathbb{R}^{n \times n}$, initial vector $x_0 \in \mathbb{R}^n$, tolerance $\epsilon$, maximum iterations $N$}
   % \ Approximation of the dominant eigenvalue $\lambda$ and eigenvector $v$
   
   \State Normalize the initial vector: $x_0 \leftarrow \frac{x_0}{\|x_0\|}$
   \For{$k = 1, 2, \dots, N$}
       \State Compute the matrix-vector product: $y_k \leftarrow A x_{k-1}$
       \State Normalize the resulting vector: $x_k \leftarrow \frac{y_k}{\|y_k\|}$
       \State Compute the Rayleigh quotient: $\lambda_k \leftarrow x_k^\top A x_k$/($x_k^\top x_k$)
       \If{$\|x_k - x_{k-1}\| < \epsilon$}
           \State \textbf{break}
       \EndIf
   \EndFor
   \State{Set $v \leftarrow x_k$ and $\lambda \leftarrow \lambda_k$}\\
   \Return $\lambda$, $v$
   \end{algorithmic}
\end{algorithm}

\subsection{The QR Iteration Algorithm}
To compute the eigendecomposition of a matrix using QR decomposition, we first factorize the matrix $\mathbf{A}$ into the product of $\mathbf{Q}$ and $\mathbf{R}$, 
then we use the QR-iteration algorithm. The QR-iteration algorithm is an iterative method that computes the eigenvalues and eigenvectors of a matrix by repeatedly applying the QR decomposition to the matrix.
The \href{https://en.wikipedia.org/wiki/QR_algorithm}{QR iteration algorithm} estimates the eigenvalues and eigenvectors of a square matrix.
\begin{mdframed}
The QR-iteration algorithm consists of two phases:
\begin{itemize}[leftmargin=16pt]
\item{\textbf{Phase 1: Eigenvalues}. Let $\mathbf{A}\in\mathbb{R}^{n\times{n}}$. 
Then, starting with $k = 0$, we compute the \href{https://en.wikipedia.org/wiki/QR_decomposition}{QR decomposition} of the matrix $\mathbf{A}_{k}$:
\begin{equation*}
\mathbf{A}_{k+1}\leftarrow\mathbf{R}_{k}\mathbf{Q}_{k} = \mathbf{Q}_{k}^{\top}\mathbf{A}_{k}\mathbf{Q}_{k}\qquad{k=0,1,\dots} 
\end{equation*}
where $\mathbf{Q}_{k}$ is an orthogonal matrix, i.e., $\mathbf{Q}^{\top}_{k} = \mathbf{Q}^{-1}_{k}$ and $\mathbf{R}_{k}$ is an upper triangular matrix. 
As $k\rightarrow\infty$, the matrix $\mathbf{A}_{k}$ converges to a triangular matrix with the eigenvalues listed along the diagonal.}
\item{\textbf{Phase 2: Eigenvectors}. We compute the eigenvectors associated with each eigenvalue by solving the homogenous system of equations:
\begin{equation*}
\left(\mathbf{A}-\lambda_{j}\mathbf{I}\right)\cdot\mathbf{v}_{j} = \mathbf{0}
\end{equation*}
where $\mathbf{I}$ is the identity matrix.}
\end{itemize}
\end{mdframed}

\subsection{Singular Value Decomposition (SVD)}
We can compute the rank (and get some other useful information) about a matrix $\mathbf{A}$ by decomposing the matrix using singular value decomposition into its singular values and singular vectors.
Singular value decomposition (SVD), originally developed in the 1870s \citep{Stewart:1993} is a matrix factorization technique that is based on the eigendecomposition of a matrix.
Suppose we have a matrix $\mathbf{A} \in \R^{m \times n}$. The SVD of $\mathbf{A}$ is a factorization of the form: $\mathbf{A} = \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\top}$, where
$\mathbf{U}\in\mathbb{R}^{n\times{n}}$ and $\mathbf{V}\in\mathbb{R}^{m\times{m}}$ are orthogonal matrices, i.e., $\mathbf{U}\cdot\mathbf{U}^{\top} = \mathbf{I}$ and $\mathbf{\Sigma}\in\mathbb{R}^{n\times{m}}$ is a diagonal matrix containing 
the singular values $\sigma_{i}=\Sigma_{ii}$. The matrix $\mathbf{A}\in\mathbb{R}^{n\times{m}}$ can be decomposed as:
\begin{equation}
\mathbf{A} = \sum_{i=1}^{r_{\mathbf{A}}}\sigma_{i}\cdot\left(\mathbf{u}_{i}\otimes\mathbf{v}_{i}\right)
\end{equation}
where $r_{\mathbf{A}}$ is the rank of matrix $\mathbf{A}$, and $\sigma_{i}$ are the singular values (ordered from largest to smallest) of the matrix $\mathbf{A}$,
and $\otimes$ denotes the outer product. 
The outer product $\hat{\mathbf{A}}_{i} = \mathbf{u}_{i}\otimes\mathbf{v}_{i}$ is a rank-1 matrix, i.e., a mode of the original matrix $\mathbf{A}$,  with elements: 
\begin{equation}
\hat{a}_{jk} = u_{j}v_{k}\qquad{j=1,2,\dots,n~\text{and}~k=1,2,\dots,m}
\end{equation}
where the vectors $\mathbf{u}_{i}$ and $\mathbf{v}_{i}$ denote the left (right) singular vectors, respectively, of the matrix $\mathbf{A}$.
Singular value decomposition is a powerful tool for analyzing the structure of a matrix, e.g., for computing properties such as the rank of a matrix, and is used in many applications, 
including data compression, image processing, and control theory. It is also used for solving linear systems of equations,
such as those that arise in linear regression tasks. The SVD is also used in a huge variety of unsupervised learning type applications, e.g., understanding gene expression data \citep{Alter:2000aa, Alter:2006},  
the structure of chemical reaction networks \citep{Famili:2003aa}, 
in process control applications \citep{MooreSVD1986}, and analysis of various type of human centered networks \citep{SASTRY20075275, 7993780}.


\bibliography{References-W4.bib}

\end{document}