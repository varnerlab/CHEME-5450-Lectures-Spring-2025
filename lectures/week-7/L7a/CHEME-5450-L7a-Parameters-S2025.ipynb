{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ad02a2f-effe-4056-9b4a-b1cca9e819e9",
   "metadata": {},
   "source": [
    "# L7a: Estimating Model Parameters\n",
    "In this lecture, we'll look at some approaches to estimate unknown model parameters from data. The key topics we'll cover are:\n",
    "\n",
    "* __Least squares minimization__ is an optimization problem used to estimate model parameters by minimizing the squared difference between experimental data and model predictions. This problem can be solved using a variety of optimization algorithms.\n",
    "* __Maximum likelihood estimation__ is a statistical method used to estimate model parameters by maximizing the likelihood of observing the experimental data given the model. This method is particularly useful when the data is noisy and the model is probabilistic.\n",
    "* __Gradient-based optimization__ is a general approach to solving optimization problems by iteratively updating the model parameters in the direction of the gradient of the objective function. This method is widely used in machine learning and scientific computing. \n",
    "* __Heuristic optimization__ is a class of optimization algorithms that do not rely on the gradient of the objective function. These algorithms are often used when the derivatives of the objective function are challenging to compute or the objective function is non-convex, i.e., many local minima exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e1aeb3",
   "metadata": {},
   "source": [
    "## Why is the parameter estimation problem hard to solve?\n",
    "Parameter estimation in systems and synthetic biology models faces several common challenges:\n",
    "\n",
    "* __Non-Identifiability and Sloppiness__: Many systems biology models exhibit non-identifiability, meaning multiple sets of parameter values can fit the data equally well. Additionally, models often display \"sloppiness,\" where parameters have sensitivities spread over many orders of magnitude, making precise estimation difficult.\n",
    "* __High-Dimensional Parameter Spaces__: Biological models can have _many_ unknown parameters, leading to high-dimensional search spaces. This complexity increases computational demands and makes optimization challenging.\n",
    "* __Noisy and Limited Data__: Experimental data used for parameter estimation in systems biology (and to a lesser extent in some synthetic applications) are often noisy and limited in scope, which complicates the optimization process by introducing additional local minima in the objective function.\n",
    "* __Overfitting__: Models with many parameters can overfit the data, capturing random fluctuations rather than underlying trends, which results in poor predictive performance on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b6932e",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "Gradient descent is a numerical search algorithm that minimizes a function by iteratively adjusting the parameters in the opposite direction of the gradient. Suppose there exists an objective function, e.g., the negative log-likelihood $\\mathcal{L}(\\theta)$ that we want to minimize with respect to parameters $\\theta\\in\\mathbb{R}^{p}$. We assume $\\mathcal{L}(\\theta)$ is _at least once differentiable_ with respect to the parameters, i.e., we can compute the gradient $\\nabla_{\\theta}{\\mathcal{L}}(\\theta)$. The gradient points in the direction of the steepest increase of the function. Thus, we can iteratively update the parameters to minimize the objective function using the update rule:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\theta_{k+1} = \\theta_{k} - \\alpha(k)\\cdot\\nabla_{\\theta}\\mathcal{L}(\\theta_{k})\\quad\\text{where}{~k = 0,1,2,\\dots}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $k$ denotes the iteration index, and $\\nabla_{\\theta}\\mathcal{L}(\\theta)$ is the gradient of the negative log-likelihood function with respect to the parameters $\\theta$.\n",
    "* __What is $\\alpha(k)$?__ The (hyper) parameter $\\alpha(k)>0$ is the _learning rate_ which can be a function of the iteration count $k$. This is a user-adjustable parameter, and we'll assume it's constant for today.\n",
    "* __Stopping?__ Gradient descent will continue to iterate until a stopping criterion is met, i.e., $\\lVert\\theta_{k+1} - \\theta_{k}\\rVert\\leq\\epsilon$ or the maximum number of iterations is reached, or some other stopping criterion is met, i.e., the gradient is small at the current iteration $\\lVert\\nabla_{\\theta}\\mathcal{L}(\\theta_{k})\\rVert\\leq\\epsilon$.\n",
    "\n",
    "Pusedocode for a naive gradient descent algorithm (for a fixed learning rate) is shown in the [CHEME 5820 lecture notes](https://github.com/varnerlab/CHEME-5820-Lectures-Spring-2025/blob/main/lectures/week-3/L3c/docs/Notes.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f651bb0",
   "metadata": {},
   "source": [
    "## Alternatives to gradient descent\n",
    "Gradient descent is a powerful optimization algorithm, but it has some limitations. For example, it can be slow to converge, especially when the objective function is non-convex or has a complex landscape. Additionally, gradient descent requires the gradient of the objective function, which can be computationally expensive to compute for large models. In these cases, alternative optimization algorithms may be more suitable. Some common alternatives to gradient descent include:\n",
    "\n",
    "* __Genetic Algorithms__: These are population-based optimization methods inspired by natural selection and genetics. They use processes like mutation, crossover, and selection to evolve a population of candidate solutions towards optimal values. Genetic algorithms are particularly useful for navigating complex search spaces without requiring gradient information, making them suitable for black-box optimization problems134.\n",
    "* __Simulated Annealing__: This algorithm mimics the annealing process in metallurgy, where the temperature is gradually lowered to achieve a stable crystal structure. It starts with a high \"temperature\" that allows for extensive exploration of the solution space and gradually cools down, reducing the likelihood of accepting worse solutions. This method is effective for avoiding local minima and can be applied to problems where gradient information is not available134.\n",
    "* __Particle Swarm Optimization (PSO)__: PSO is another population-based method that involves a swarm of particles moving through the search space. Each particle adjusts its position based on its own experience and the experience of its neighbors, leading to a collective convergence towards optimal solutions. PSO is known for its ability to handle complex optimization problems and can outperform gradient descent in certain scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6251add9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
